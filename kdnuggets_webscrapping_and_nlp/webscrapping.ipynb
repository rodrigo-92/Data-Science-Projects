{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kdnuggets.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse response\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# find content\n",
    "content = soup.find_all('div', {'id': 'content'})\n",
    "\n",
    "# first 'content' tag contains latest posts, partners' posts, top posts past 30 days, recent posts\n",
    "tables_content = content[0].find_all('table', {'class': 'thb'})\n",
    "\n",
    "# first tables content contains latest posts, so only grab tags including the right url\n",
    "latest_posts = tables_content[0].find_all(href=re.compile(\".html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping following url: https://www.kdnuggets.com/2023/02/getting-started-python-generators.html\n",
      "Title: Getting Started with Python Generators\n",
      "Excerpt: Learn about Python generators and write memory-efficient and Pythonic code.\n",
      "Author: Bala Priya C\n",
      "Content: Learning how to work with Python generators can help you write more Pythonic and efficient code. Using generators can be especially useful when you need to work with large sequences.\n",
      "In this tutorial, you’ll learn how to use generators in Python by defining generator functions and generator expressions. You’ll then learn how using generators can be a memory-efficient choice. \n",
      " \n",
      "Defining Generator Functions in Python\n",
      " \n",
      "To understand how a generator function is different from a normal Python function, let's start with a regular Python function and then rewrite it as a generator function.\n",
      "Consider the following function get_cubes(). It takes in a number num as the argument and returns the list of cubes of the numbers 0, 1, 2 up to num -1:\n",
      "  \n",
      "The above function works by looping through the list of numbers 0, 1, 2, up to num -1 and appending the cube of each number to the cubes list. Finally, it returns the cubes list. \n",
      "You can already tell this is not the recommended Pythonic way to create a new list. Instead of looping through using a for loop and using the append() method, you can use a list comprehension expression. \n",
      "Here is the equivalent of the function get_cubes() that uses list comprehension instead of an explicit for loop and the append() method:\n",
      "  \n",
      "Next let’s rewrite this function as a generator function. The following code snippet shows how the get_cubes() function can be rewritten as a generator function get_cubes_gen():\n",
      "  \n",
      "From the function definition, you can tell the following differences:\n",
      "\n",
      "We have the yield keyword instead of the return keyword.\n",
      "We are not returning a sequence or populating an iterable such as a Python list to get the sequence.\n",
      "\n",
      "So how does the generator function work? To understand, let’s call the above-defined functions and take a closer look.\n",
      " \n",
      "Understanding Function Calls\n",
      " \n",
      "Let us call the get_cubes() and get_cubes_gen() functions and see the differences in the respective function calls.\n",
      "When we call the get_cubes() function  with the number 6 as the argument, we get the list of cubes as expected. \n",
      "  \n",
      "  \n",
      "Now call the generator the function with the same number 6 as the argument and see what happens. You can call the generator function get_cubes_gen() just the way you would call a normal Python function. \n",
      "  \n",
      "If you print out the value of cubes_gen(), you’ll get a generator object as opposed to the entire resultant list that contains the cube of each of the numbers.\n",
      "  \n",
      "So how do you access the elements of the sequence? To code along, start a Python REPL and import the generator function. Here, I have my code in the gen_example.py file, so I’m importing the get_cubes_gen() function from the get_cubes_gen() module. \n",
      "  \n",
      "You can call next() with the generator object as the argument. Doing so returns 0, the first element in the sequence\n",
      "  \n",
      "Now when you call next() again, you’ll get the next element in the sequence, which is 1.\n",
      "  \n",
      "To access the subsequent elements in the sequence, you can continue to call next(), as shown: \n",
      "  \n",
      "For num = 6, the resultant sequence is the cube of the numbers 0, 1, 2, 3, 4, and 5. Now that we’ve reached 125, the cube of 5, what happens when you call next again? \n",
      "We see that a StopIteration exception is raised. \n",
      "  \n",
      "Under the hood, the generator function executes until the execution reaches the yield statement, and the control returns to the call site. However, unlike a normal Python function that returns control to the call site once the return statement, a generator function suspends execution temporarily. And it keeps track of its state that helps us get the subsequent elements by calling next(). \n",
      "You can also loop through the generator object using a for loop. The control exits the loop when the StopIteration exception is raised (that’s how for loops work under the hood). \n",
      "  \n",
      "  \n",
      "Generator Expressions in Python\n",
      " \n",
      "Another common way to use generators is using generator expressions. Here’s  the generator expression equivalent of the get_cubes_gen() function:\n",
      "  \n",
      "The above generator expression may look similar to list comprehension, except for the use of () in place of []. However, as discussed, the following key differences hold:\n",
      "\n",
      "A list comprehension expression generates the entire list and stores it in memory.\n",
      "The generator expression, on the other hand, yields the elements of the sequence on demand.\n",
      "\n",
      " \n",
      "Python Generators vs. Lists: Understanding Performance Improvements\n",
      " \n",
      "In the sample function call in the previous section, we generated a sequence of cubes of the numbers zero through five. For such small sequences, using a generator may not give you significant performance gains. However, generators are certainly a memory-efficient choice when you work with longer sequences. \n",
      "To see this in action, generate the sequence of cubes for value of num in a  wider range:\n",
      "  \n",
      "Now let us print out the size of the size in memory of the static list and the generator object for the when num changes (as in the snippet above):\n",
      "  \n",
      "From the output, we see that the generator object has a constant memory footprint unlike a list where the memory grows with num.This is because a generator performs lazy evaluation and yields the subsequent values in the sequence on demand. It does not compute all the values ahead of time.\n",
      "  \n",
      "To get a better idea of how the sizes of the static list and generator change with change in the value of num, we can plot the values of num and the sizes of the list and the generators, as shown below: \n",
      " \n",
      "\n",
      "  \n",
      "In the graph above, we see that when num increases, the size of the generator is constant, whereas the size of the list is prohibitively large.\n",
      " \n",
      "Conclusion\n",
      " \n",
      "In this tutorial, you’ve learned how generators work in Python. The next time you need to work with a large file or dataset, you can consider using generators to iterate efficiently over it. When you use generators, you can iterate over the generator object, read in a line or a small chunk, process it or apply transformations as needed—without having to store the original dataset in memory. However, keep in mind that you cannot store such values in memory for processing at a later time. If you need to, you’ll have to use lists.\n",
      " \n",
      " \n",
      "Bala Priya C is a technical writer who enjoys creating long-form content. Her areas of interest include math, programming, and data science. She shares her learning with the developer community by authoring tutorials, how-to guides, and more.\n",
      " \n",
      "Scrapping following url: https://www.kdnuggets.com/2023/02/top-posts-week-0220-0226.html\n",
      "Title: Top Posts February 20-26: 5 SQL Visualization Tools for Data Engineers\n",
      "Excerpt: 5 SQL Visualization Tools for Data Engineers • Free TensorFlow 2.0 Complete Course • The Importance of Probability in Data Science • 4 Ways to Rename Pandas Columns • 5 Statistical Paradoxes Data Scientists Should Know\n",
      "Author: KDnuggets\n",
      "Content: \n",
      "\n",
      "\n",
      "\n",
      "Most Popular Posts Last Week\n",
      "\n",
      "\n",
      "\n",
      "5 SQL Visualization Tools for Data Engineers by Ndz Anthony\n",
      "Free TensorFlow 2.0 Complete Course by Kanwal Mehreen\n",
      "The Importance of Probability in Data Science by Nisha Arya\n",
      "4 Ways to Rename Pandas Columns by Abid Ali Awan\n",
      "5 Statistical Paradoxes Data Scientists Should Know by Nate Rosidi\n",
      "\n",
      "\n",
      "Most Popular Posts Past 30 Days\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The ChatGPT Cheat Sheet by KDnuggets\n",
      "SQL and Python Interview Questions for Data Analysts by Nate Rosidi\n",
      "4 Ways to Rename Pandas Columns by Abid Ali Awan\n",
      "ChatGPT as a Python Programming Assistant by Matthew Mayo\n",
      "Learn Machine Learning From These GitHub Repositories by Nisha Arya\n",
      "How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at and .iat by Manu Jeevan\n",
      "5 Free Tools For Detecting ChatGPT, GPT3, and GPT2 by Abid Ali Awan\n",
      "Top Free Resources To Learn ChatGPT by Abid Ali Awan\n",
      "Learn Data Engineering From These GitHub Repositories by Nisha Arya\n",
      "20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT Edition, Part 2 by Matthew Mayo\n",
      "\n",
      "\n",
      "\n",
      "Scrapping following url: https://www.kdnuggets.com/2023/02/top-5-advantages-catboost-ml-brings-data-make-purr.html\n",
      "Title: Top 5 Advantages That CatBoost ML Brings to Your Data to Make it Purr\n",
      "Excerpt: This article outlines the advantages of CatBoost as a GBDTs for interpreting data sources that are highly categorical or contain missing data points.\n",
      "Author: Gellért Nacsa\n",
      "Content: Businesses worldwide are increasingly expecting their decision-making processes to be informed by data-driven predictive analytics systems, or at least they should be. From timely investment to fulfillment logistics to fraud prevention, data is being leveraged in an increasing number of workflows, with an increasing number of eyes on it. In other words, streams of information that were often relegated to data scientists are now being viewed – and manipulated – by people less savvy in the data dark arts.\n",
      "Today, relative data laypersons may suddenly find themselves with the responsibility to both contribute to data pools and interpret their ML analysis of them. This could lead to situations that make your data spit and hiss at you, instead of purring cooperatively. To mitigate the potential of scratches from poor data hygiene bites from overfitting, or worse, CatBoost leaps up onto the kitchen table, clearly presenting itself as the best option for many verticals.\n",
      " \n",
      "The Benefits of Owning a Cat(Boost)\n",
      " \n",
      "CatBoost is a gradient-boosting ML system that sets itself apart from other GBDTs by offering unique solutions for interpreting data sources that are highly categorical or contain missing data points. Indeed, this is where the API gets its name, from Boosting Categories, rather than from some feline origin. Side-by-side comparisons show that CatBoost also outperforms XGBoost and LightBGM significantly in terms of prediction delivery time and parameter tuning time, while remaining comparable in other metrics. \n",
      "Under the hood, CatBoost achieves this by using the split-by-popularity method to create symmetrical decision trees.  By grouping features into a single split with only a left and right child, the necessary processing power and time are greatly reduced when compared to trees with children for each individual feature in a set. These features could be categorical or numerical. They can then be parsed at different levels of the tree until the target variable is met. This effectively regularizes the data, discouraging data points from developing explicit correlations by offering multiple viewpoints on the same subsets. This gives CatBoost’s predictions more resilience from both overfitting and generalization errors in the training process.\n",
      "While these points are obviously advantageous from a data science and computational perspective, their benefits across an entire organization might be less apparent.\n",
      " \n",
      "CatBoost, An Operational Support Animal\n",
      " \n",
      "So, it doesn’t greet you with your slippers when you arrive at home, it doesn’t return the care and attention you give it in an affectionate way, so how exactly does CatBoost improve your operations? How does CatBoost’s computational efficiency actually help with essential business functions, and for whom? There are certain elements of CatBoost’s algorithm that seem like small differences compared to similar ML systems, but translate into huge logistical dividends for many businesses. \n",
      "We’re going to zoom in on several such components, then zoom back out to see how their real-world implementations help streamline processes and ultimately save money – saving money is a language everyone in the company speaks. Businesses might recognize some of their granular pain points being addressed as we explore, especially if they got scratched, or even scarred.\n",
      " \n",
      "5 Ways CatBoost Can Housebreak Your Data\n",
      " \n",
      "For each of these nuances within the open-source API, there are practical benefits that are immediately obvious to data scientists. For the increasing number of non-tech teams being asked to participate in data analysis chain of their companies, as well as those people whose job it is to approve ML workflows, it is decidedly less obvious. \n",
      " \n",
      "1: No need for one-hot encoding\n",
      " \n",
      "CatBoost handles categorical features in plaintext, avoiding the need for extensive preprocessing, as with other GBDTs that insist on numerical inputs. The most common preprocessing method for categorical data is labeling it though one-hot encoding, which breaks data into a binary, though in some cases using a dummy, ordinal, LOO, or Bayesian target encoding method is more applicable, depending on the nature of the data. Categorical features preprocessed via one-hot encoding can often be expansive and become overly dimensional quickly. Through CatBoost’s in-built ordinal encoding system, categorical features are assigned a numerical value, but the long processing times and overfitting characteristics of highly dimensional data are avoided. \n",
      "Though these calculations might happen in the cloud, the on-the-ground benefits include:\n",
      "\n",
      "As the API handles text-based categorical data natively, there are less calculations and less transformations of the data that are required, which results in less error.\n",
      "Compared to one-hot encoding, CatBoost breaks data into less features, increasing simplicity and ultimately interpretability for analyst teams.\n",
      "Text-based data that gets fed into the library gets returned with the same text label, also streamlining interpetability.\n",
      "\n",
      "\n",
      "CatBoost facilitates deep analysis by making it easy to focus on a single feature’s contribution to the model, rather than a specific value of a specific feature, as is the case with one-hot encoded columns. Using the get_feature_importance()method, CatBoost measures the features using SHAP values, which can also be easily visualized for interpretation and analysis (see below). \n",
      "By the same token, this usage of categorical data allows staff from across departments the ability to both enter and interpret data much more easily. \n",
      "This level of accessibility allows companies to have unified data streams, so all arms of the company are referring to a single pool of information. Companies that have been bitten by interdepartmental confusion from having multiple profiles of a single entity know how many stresses and resources this can save.\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "CatBoost natively hosts the SHAP library, facilitating output visualizations like these, which were trained on fraud prevention data.\n",
      "This aspect of CatBoost’s API is an obvious advantage for verticals working with data that is inherently less numerical than, say, financial forecasting. Areas like medical diagnosis, fraud prevention, market segmentations, and advertising are all fields that deal with large amounts of categorical data, and would benefit from CatBoost’s deft claws at dealing with it.\n",
      " \n",
      "2: Oblivious trees\n",
      " \n",
      "\n",
      "  \n",
      "CatBoost leverages oblivious decision trees as the base learner in its gradient boosting process. This means that each tree is limited to a single symmetrical split, with either side being balanced by the algorithm’s built-in feature importance measurement. \n",
      "Rather than have nodes split multiple times for a highly dimensional feature, as with XGBoost, CatBoost instead creates multiple levels to analyze a single feature. This gives it the ability to shine when it comes to:\n",
      "\n",
      "Optimizing processing time and power. Highly dimensional non-numerical data will inevitably require more processing time and power when pushed through XGBoost compared to CatBoost. A single split in each node means significantly faster processing speed for each decision, cutting down on energy usage and optimizing wait times, internally or externally.\n",
      "Regularization and robustness against overfitting. Algorithms that take features and split trees into all possible outcomes of that feature will notice their data overfitting often. By limiting the complexity of each level of the tree, it essentially creates a more rigid regression line through your data, a sort of de facto regularization. This results in less false negatives and false positives, which could translate into how your organization uses its data in any number of ways.\n",
      "Ease of interpretability. Again, depending on the industry and what process is being automated, there may be good reason to be able to explain how a given ML determination was made. This could be to satisfy a customer, but could also be part of legal compliance when it comes to having transparently fair practices or performing safety due diligence. \n",
      "\n",
      "The oblivious tree model of CatBoost is what gives it most of its processing lightness, and also makes it a highly scalable option for large datasets. When handling categorical data, CatBoost is much more efficient at drawing conclusions quickly while also disregarding data noise. In the fields of market segmentation, automated supply chain optimization, PPC advertising, for example, noisy data is sometimes the reason a seemingly random advertisement gets targeted at your device, or a chain store has too much or too little of something.\n",
      " \n",
      "3: Data “Nan Handling”\n",
      " \n",
      "Datasets with missing values are, of course, less useful for analysis in general. When approaching numerical datasets, tather than let those missing values affect the stability of the model, CatBoost automatically replaces the missing values. Depending on the size of the subset, the value is either replaced through simple calculations, or else through machine-learning derived relationships between features. Those relationships may be only borderline explainable, but the explainable benefits in actual work processes include:\n",
      "\n",
      "Better performance in terms of accuracy and false positive rates, turning into smoother processes anywhere data is being analyzed by CatBoost.\n",
      "In the case of fraud prevention, null values can themselves be useful as indicators of potential risk.\n",
      "Facilitates bespoke models for individual customers, as each will have different processes, and will have different data and nan ratios (rate of Not a Number entries in datasets).\n",
      "Other verticals like healthcare, banking, customer analytics, and supply chain management make good use of nan handling – consider anywhere someone on the data entry chain, from new customers to fulfillment agents, might be inclined to not fill in part of a form. Sometimes this could be laziness, other times, it could indicate attempted application fraud.  \n",
      "\n",
      "Notably, CatBoost only handles numerical data nan handling natively. For categorical data, an individual category along the lines of “empty string” or “missing value” should be created in order for these NaN values to be considered in the symmetrical split.\n",
      "Here’s an example of how to easily handle the nan values in categorical data:\n",
      " \n",
      "4: Parallel Processing (Zoomies) \n",
      " \n",
      "As mentioned above, CatBoost’s single, symmetrical split is greatly beneficial to processing power and prediction speed. \n",
      "There are two main reasons for this. The first is the symmetrical tree structure’s ability to reach a prediction through vector multiplication. Where other gradient-boosting libraries would perform these calculations at each level of the tree, CatBoost can apply it to the entire tree, drastically speeding up prediction generation. \n",
      "The second explanation for CatBoost’s speed is its sort-by-popularity feature, which takes dimensional data and groups it into two equal branches, as opposed to creating a possible branch for each feature. Naturally, a single split takes less energy to navigate than 5, but having this kind of organization lends itself easy to parallel processing – splitting data into subsets to divide the task across multiple processors. Depending on the available hardware resources, CatBoost can also find the method of parallel processing that is most optimal.\n",
      "Apart from the obvious benefits of lower energy consumption and faster overall processes, consider some ways that customer and partner experiences might be affected by inefficient data handling:\n",
      "\n",
      "Internal workflows are often fraught with small wait times that can balloon into larger ones. We all know this.\n",
      "Small wait times for risk analysis, fraud prevention, age verification, and other security measures can make or break an online experience. Churn obviously increases as high-friction wait times do.\n",
      "\n",
      " \n",
      "5: Cuteness factor\n",
      " \n",
      "CatBoost is quite endearing compared to similar ML algorithms. Regardless of the level of machine learning or data expertise, the API can be learned with its clear and understandable documentation. Consider these features together:\n",
      "\n",
      "Explains hypertuning parameters to understand what specific problem is being solved\n",
      "Automatically handles categorical data including pre-processing\n",
      "Has a robust model that is generally deployable out-of-box\n",
      "Advanced visualizations\n",
      "\n",
      "  \n",
      "The code block above showcases how clean and easy a CatBoost model can be trained on a boston dataset. This is a good example of how CatBoost creates a more approachable environment than some, especially considering the large and active community around the product.\n",
      "The benefits of this approachability should be clear, particularly for people with experience onboarding themselves to new pieces of advanced business software. In general, being able to include a wider range of team members in data processes without scaring them off is valuable for distributing responsibilities and optimizing resource distribution.\n",
      " \n",
      "Are You A CatBoost Person or…\n",
      " \n",
      "Though it was designed with speed and efficiency for categorical data specifically, CatBoost handles numerical data with the same deftness. That said, there are some areas that could certainly be just as well-suited with XGBoost or LightGBM. These two algorithms were designed with speed and scalability in mind, including purely numerical data that is highly dimensional and constantly changing. So verticals that deal with very large datasets that are inherently complete and in flux, may do better with one of these two APIs, like:\n",
      "\n",
      "financial forecasting\n",
      "energy distribution\n",
      "supply chain management\n",
      "\n",
      "LightGBM in particular thrives in places where processing speed is critical, such as natural language processing and image recognition. \n",
      "As data becomes more omnipresent within the business functions of an increasingly digitized corporate space, data solutions need to be more accessible. Verticals like advertising, market analysis, customer segmentation, fraud prevention, and medical treatment, CatBoost is likely a good match. Many kinds of finance, too. Though it may never be as accessible as an actual housecat, in many cases CatBoost may be more friendly, and certainly more useful.\n",
      " \n",
      " \n",
      "Gellért Nacsa is the Data Science Lead at SEON. He studied applied mathematics at university and worked as a data analyst, algorithm designer, and data scientist. He enjoys playing with data, machine learning, and learning new stuff all the time.\n",
      " \n",
      "Scrapping following url: https://www.kdnuggets.com/2023/02/data-warehousing-etl-best-practices.html\n",
      "Title: Data Warehousing and ETL Best Practices\n",
      "Excerpt: How you can improve your data warehousing ETL process with these simple practices.\n",
      "Author: Nisha Arya\n",
      "Content:  \n",
      "What is a Data Warehouse?\n",
      " \n",
      "A Data warehouse is a central repository that contains data, information, and other variables that can be analyzed to help businesses make informed decisions. For example, it can be used to measure performance or acquire validations.\n",
      "It involves the maintenance of historical data which then benefits knowledge workers and others in the organization in their decision-making process. Data Warehouses provide companies with:\n",
      "\n",
      "A single source of truth\n",
      "Consistency\n",
      "Effective decision-making process\n",
      "\n",
      " \n",
      "What is ETL?\n",
      " \n",
      "ETL stands for EXTRACT, TRANSFORM and LOAD. It is the process of moving data from multiple sources to a centralized single database. It starts with the raw data being EXTRACTED from the source, and then TRANSFORMED on a separate processing server, in which it is then LOADED into the target database. \n",
      " \n",
      "Common Mistakes in Data Warehousing and ETL\n",
      " \n",
      "Here is a list of the common mistakes that people face with Data Warehouses and ETL processes:\n",
      "\n",
      "A lack of understanding of all source data\n",
      "Lack of historical data\n",
      "Too much time is spent on profiling the source data\n",
      "Too much time is spent on testing the extract process\n",
      "Agreeing to a set of rules\n",
      "Not logging the ETL process\n",
      "Not being open to new technology\n",
      "\n",
      " \n",
      "ETL Best Practises\n",
      " \n",
      "1. Create a Roadmap for Your ETL Processes\n",
      " \n",
      "With anything you do in life, it is better to start with a plan rather than diving into the deep end. You may want to write it down or you make want to create a visualisation of your process. But the roadmap is essential as it allows you to go back and make adjustments and learn via trial and error.\n",
      " \n",
      "2. Populate Test Data Early\n",
      " \n",
      "As you create your roadmap, you will be considering the end goal in mind. In your ETL process, you want to understand ‘what data model do you want to populate?’. Populating your data warehouse with sample data that is related to your end goal will make your process more effective. It helps you to keep in line with the task at hand and create rules. \n",
      " \n",
      "3. Review Source Data and Systems\n",
      " \n",
      "The source system contains the data that is fed to the data warehouse. You can use profiling tools to help you identify NULL values or what the columns serve as. Rather than spending your time on profiling queries, reviewing your source system can improve your ETL process. \n",
      "You need to identify primary key definitions in every source table, and any possible information/data related to it. Use this practice as a verification stage of feeling confident about what you are feeding into your data warehouse.\n",
      " \n",
      "4. Data Type Issues\n",
      " \n",
      "When querying your data, you don’t want to be coming across various errors due to data type issues. It’s a problem that should be addressed early on in the process so that it doesn’t cause problems later on. \n",
      " \n",
      "5. Extracting the Data\n",
      " \n",
      "Extracting the data from source systems is an important phase, which can cause many problems if not done correctly. Here are a few tips:\n",
      "\n",
      "Having a timestamp column in your source system will allow you to rely on that transaction date and ensure that all the necessary data has been extracted. \n",
      "Extracting the data in incremental steps will help when you’re working with a very large source table. \n",
      "Take note of how long your extract processes take, as there may be ways you can improve it. \n",
      "\n",
      "All your extracting data processes should be thoroughly reviewed and verified. \n",
      " \n",
      "6. Collating All Activity in ETL Logs\n",
      " \n",
      "One of the best practise, not only with data warehouses but in life is logging everything. It’s better to go back to a whiteboard that had different ideas and processes scribbled everywhere, than a blank one. \n",
      "Through ETL logs, you can find valuable information such as extraction time, changes in rows, errors and more. \n",
      " \n",
      "7. Alerts\n",
      " \n",
      "It can be overwhelming to watch an ETL process occur. You want to keep an eye on it, but sometimes it can take longer than you think and you might catch yourself up at ungodly hours. Some companies have created a messaging and alert procedure, which notifies them of any fatal errors that they need to be aware of.\n",
      " \n",
      "Conclusion\n",
      " \n",
      "Although some may say these are practise that everybody should be doing when working with data warehouses and ETL. It will surprise you how these are the main challenges that a lot of companies/teams face. \n",
      "If you would like to learn more about Data Warehousing and ETL, have a read of:\n",
      "\n",
      "Why Organizations Need Data Warehouses\n",
      "SQL and Data Integration: ETL and ELT\n",
      "ETL vs ELT: Data Integration Showdown\n",
      "\n",
      " \n",
      " \n",
      "Nisha Arya is a Data Scientist and Freelance Technical Writer. She is particularly interested in providing Data Science career advice or tutorials and theory based knowledge around Data Science. She also wishes to explore the different ways Artificial Intelligence is/can benefit the longevity of human life. A keen learner, seeking to broaden her tech knowledge and writing skills, whilst helping guide others.\n",
      " \n",
      "Scrapping following url: https://www.kdnuggets.com/2023/02/pyspark-data-science.html\n",
      "Title: PySpark for Data Science\n",
      "Excerpt: In this tutorial, we will learn to Initiates the Spark session, load, and process the data, perform data analysis, and train a machine learning model.\n",
      "Author: Abid Ali Awan\n",
      "Content: PySpark is an Python interference for Apache Spark. It is an open-source library that allows you to build Spark applications and analyze the data in a distributed environment using a PySpark shell. You can use PySpark for batch processing, running SQL queries, Dataframes, real-time analytics, machine learning, and graph processing. \n",
      "Advantages of using Spark:\n",
      "\n",
      "In-memory caching allows real-time computation and low latency.\n",
      " It can be deployed using multiple ways: Spark’s cluster manager, Mesos, and Hadoop via Yarn.\n",
      "User-friendly API is available for all popular languages that hide the complexity of running distributed systems.\n",
      "It is 100x faster than Hadoop MapReduce in memory and 10x faster on disk.\n",
      "\n",
      "In this code-based tutorial, we will learn how to initial spark session, load the data, change the schema, run SQL queries, visualize the data, and train the machine learning model. \n",
      " \n",
      "Getting Started\n",
      " \n",
      "If you want to use PySpark on a local machine, you need to install Python, Java, Apache Spark, and PySpark. If you want to avoid all of that, you can use Google Colab or Kaggle. Both platforms come with pre-installed libraries, and you can start coding within seconds.  \n",
      "In the Google Colab Notebook, we will start by installing pyspark and py4j.\n",
      "  \n",
      "After that, we will need to provide the session name to initialize the Spark session. \n",
      "  \n",
      "Importing the Data using PySpark\n",
      " \n",
      "In this tutorial, we will be using Global Spotify Weekly Chart from Kaggle. It contains information about the artist and the songs on the Spotify global weekly chart.\n",
      "Just like Pandas, we can load the data from CSV to dataframe using spark.read.csv function and display Schema using printSchema() function.\n",
      "  \n",
      "Output:\n",
      "As we can observe, PySpark has loaded all of the columns as a string. To perform exploratory data analysis, we need to change the Schema. \n",
      "  \n",
      "Updating the Schema \n",
      " \n",
      "To change the schema, we need to create a new data schema that we will add to StructType function. You need to make sure that each column field is getting the right data type. \n",
      "  \n",
      "Then, we will load the CSV files using extra argument schema. After that, we will print the schema to check if the correct changes were made.\n",
      "  \n",
      "Output:\n",
      "As we can see, we have different data types for the columns. \n",
      "  \n",
      "Exploring the Data\n",
      " \n",
      "You can explore your data as a dataframe by using toPandas() function. \n",
      " \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "Just like pandas, we can use describe() function to display a summary of data distribution. \n",
      "  \n",
      "\n",
      "  \n",
      "The count() function used for displaying number of rows. Read Pandas API on Spark to learn about similar APIs.\n",
      "  \n",
      "Column Manipulation\n",
      " \n",
      "You can rename your column by using withColumnRenamed function. It requires an old name and a new name as string. \n",
      "  \n",
      "\n",
      "  \n",
      "To drop single or multiple columns, you can use drop() function.\n",
      "  \n",
      "\n",
      "  \n",
      "You can use .na for dealing with missing valuse. In our case, we are dropping all missing values rows. \n",
      " \n",
      "Data Analysis\n",
      " \n",
      "For data analysis, we will be using PySpark API to translate SQL commands. In the first example, we are selecting three columns and display the top 5 rows. \n",
      "  \n",
      "\n",
      "  \n",
      "For more complex queries, we will filter values where ‘Total’ is greater than or equal to 600 million to 700 million. \n",
      " \n",
      "\n",
      "Note: you can also use df.Total.between(600000000, 700000000) to filter out records. \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "Write if/else statement to create a categorical column using when function.\n",
      "  \n",
      "\n",
      "  \n",
      "You can use all of the SQL commands as Python API to run a complete query. \n",
      "  \n",
      "\n",
      "  \n",
      "Data Visualization\n",
      " \n",
      "Let’s take above query and try to display it as a bar chart. We are plotting “artists v.s average song streams” and we are only displaying the top seven artists. \n",
      "It is awesome, if you ask me. \n",
      "  \n",
      "\n",
      "  \n",
      "Saving the Result\n",
      " \n",
      "After processing the data and running analysis, it is the time for saving the results. \n",
      "You can save the results in all of the popular file types, such as CSV, JSON, and Parquet. \n",
      "  \n",
      "Data Pre-Processing\n",
      " \n",
      "In this section, we are preparing the data for the machine learning model. \n",
      "\n",
      "Categorical Encoding: converting the categorical columns into integers using StringIndexer.\n",
      "Assembling Features: assembling important features into one vector column. \n",
      "Scaling: scaling the data using the StandardScaler scaling function. \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "KMeans Clustering\n",
      " \n",
      "I have already run the Kmean elbow method to find k. If you want to see all of the code sources with the output, you can check out my notebook.\n",
      "Just like scikit-learn, we will provide a number of clusters and train the Kmeans clustering model. \n",
      "  \n",
      "\n",
      "  \n",
      "Visualizing Predictions\n",
      " \n",
      "In this part, we will be using a matplotlib.pyplot.barplot to display the distribution of 4 clusters. \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Wrapping Up\n",
      " \n",
      "In this tutorial, I have given an overview of what you can do using PySpark API. The API allows you to perform SQL-like queries, run pandas functions, and training models similar to sci-kit learn. You get the best of all worlds with distributed computing. \n",
      "It outshines a lot of Python packages when dealing with large datasets (>1GB). If you are a programmer and just interested in Python code, check our Google Colab notebook. You just have to download and add the data from Kaggle to start working on it.\n",
      "Do let me know in the comments, if you want me to keep writing code based-tutorials for other Python libraries.\n",
      " \n",
      " \n",
      "Abid Ali Awan (@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in Technology Management and a bachelor's degree in Telecommunication Engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness.\n",
      " \n",
      "Scrapping following url: https://www.kdnuggets.com/2023/02/top-posts-jan-2023.html\n",
      "Title: KDnuggets Top Posts for January 2023: The ChatGPT Cheat Sheet\n",
      "Excerpt: The ChatGPT Cheat Sheet • ChatGPT as a Python Programming Assistant • Python Matplotlib Cheat Sheets • Learn Machine Learning From These GitHub Repositories • ChatGPT: Everything You Need to Know • 7 Best Platforms to Practice SQL • Top Posts January 23-29: The ChatGPT Cheat Sheet • Free Data Management with Data Science Learning with CS639\n",
      "Author: KDnuggets\n",
      "Content: \n",
      " \n",
      "It may be a little later than hoped, but we finally have the January top posts calculated and ready to report.\n",
      "Topics of interest last month included ChatGPT (unsurprisingly), along with learning resources for machine learning, data management, SQL, Matplotlib, and more. Check out the articles below, and be sure to check out what we have in story for next month.\n",
      " \n",
      "\n",
      "The ChatGPT Cheat Sheet by KDnuggets\n",
      "ChatGPT as a Python Programming Assistant by Matthew Mayo\n",
      "Python Matplotlib Cheat Sheets by Sara Metwalli\n",
      "Learn Machine Learning From These GitHub Repositories by Nisha Arya\n",
      "ChatGPT: Everything You Need to Know by Nisha Arya\n",
      "7 Best Platforms to Practice SQL by Bala Priya C\n",
      "Top Posts January 23-29: The ChatGPT Cheat Sheet by KDnuggets\n",
      "Free Data Management with Data Science Learning with CS639 by Nisha Arya\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def get_header_info(post_soup: bs4.BeautifulSoup) -> tuple:\n",
    "    title = post_soup.find('div',{'id':'post-header'}).h1.text.replace('\\n', '')\n",
    "    excerpt = post_soup.find('div',{'id':'post-header'}).p.text.replace('\\n', '')\n",
    "    author = post_soup.find('div', {'id':'post-header'}).a.text.replace('\\n', '')\n",
    "\n",
    "    return (title, excerpt, author)\n",
    "\n",
    "def get_post_content(post_soup: bs4.BeautifulSoup) -> str:\n",
    "    tag_texts = []\n",
    "    for tag in post_soup.find('div',{'id':'post-'}):\n",
    "        if isinstance(tag, bs4.Tag):\n",
    "            if tag.name in ['div']:\n",
    "                tag.decompose()\n",
    "                continue\n",
    "            if tag.find(['font']):\n",
    "                tag.decompose()\n",
    "                continue\n",
    "            tag_texts.append(tag.text)\n",
    "    post_content = '\\n'.join(tag_texts)\n",
    "    return post_content\n",
    "\n",
    "for url_tag in latest_posts:\n",
    "    url = url_tag['href']\n",
    "    print(f'Scrapping following url: {url}')\n",
    "\n",
    "    post = requests.get(url)\n",
    "    post_soup = bs4.BeautifulSoup(post.text, 'html.parser')\n",
    "\n",
    "    title, excerpt, author = get_header_info(post_soup)\n",
    "\n",
    "    print(f'Title: {title}')\n",
    "    print(f'Excerpt: {excerpt}')\n",
    "    print(f'Author: {author}')\n",
    "\n",
    "    post_content = get_post_content(post_soup)\n",
    "\n",
    "    print(f\"Content: {post_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c633461e12ddbd51a02e1735af6afd1d7dd20effaf171e9e2afa76d128a40c4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
