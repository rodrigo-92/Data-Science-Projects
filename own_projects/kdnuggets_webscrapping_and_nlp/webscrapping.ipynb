{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "import pymongo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request website to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kdnuggets.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap header information and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse response\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# find content\n",
    "content = soup.find_all('div', {'id': 'content'})\n",
    "\n",
    "# first 'content' tag contains latest posts, partners' posts, top posts past 30 days, recent posts\n",
    "tables_content = content[0].find_all('table', {'class': 'thb'})\n",
    "\n",
    "# first tables content contains latest posts, so only grab tags including the right url\n",
    "latest_posts = tables_content[0].find_all(href=re.compile(\".html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header_info(post_soup: bs4.BeautifulSoup) -> tuple:\n",
    "    title = post_soup.find('div',{'id':'post-header'}).h1.text.replace('\\n', '')\n",
    "    excerpt = post_soup.find('div',{'id':'post-header'}).p.text.replace('\\n', '')\n",
    "    author = post_soup.find('div', {'id':'post-header'}).a.text.replace('\\n', '')\n",
    "\n",
    "    return (title, excerpt, author)\n",
    "\n",
    "def get_post_content(post_soup: bs4.BeautifulSoup) -> str:\n",
    "    tag_texts = []\n",
    "    \n",
    "    for tag in post_soup.find('div',{'id':'post-'}):\n",
    "\n",
    "        if isinstance(tag, bs4.Tag):\n",
    "            if tag.name in ['div']:\n",
    "                tag.decompose()\n",
    "                continue\n",
    "            if tag.find(['font']):\n",
    "                tag.decompose()\n",
    "                continue\n",
    "            tag_texts.append(tag.text)\n",
    "    post_content = '\\n'.join(tag_texts)\n",
    "    return post_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping following url: https://www.kdnuggets.com/2023/03/hydra-configs-deep-learning-experiments.html\n",
      "Scrapping following url: https://www.kdnuggets.com/2023/03/top-free-courses-large-language-models.html\n",
      "Scrapping following url: https://www.kdnuggets.com/2023/03/time-series-forecasting-statsmodels-prophet.html\n",
      "Scrapping following url: https://www.kdnuggets.com/2023/03/key-issues-associated-classification-accuracy.html\n",
      "Scrapping following url: https://www.kdnuggets.com/2023/03/top-posts-week-0227-0305.html\n",
      "Scrapping following url: https://www.kdnuggets.com/2023/03/getting-started-github-cli.html\n"
     ]
    }
   ],
   "source": [
    "post_scrappings = []\n",
    "\n",
    "for url_tag in latest_posts:\n",
    "    url = url_tag['href']\n",
    "    print(f'Scrapping following url: {url}')\n",
    "\n",
    "    # request website\n",
    "    post = requests.get(url)\n",
    "    post_soup = bs4.BeautifulSoup(post.text, 'html.parser')\n",
    "    dict_post_scrapping = {}\n",
    "\n",
    "    # get header info\n",
    "    title, excerpt, author = get_header_info(post_soup)\n",
    "\n",
    "    dict_post_scrapping['Title'] = title\n",
    "    dict_post_scrapping['Excerpt'] = excerpt\n",
    "    dict_post_scrapping['Author'] = author\n",
    "\n",
    "    # get post content info\n",
    "    post_content = get_post_content(post_soup)\n",
    "\n",
    "    dict_post_scrapping['Content'] = post_content\n",
    "\n",
    "    # append scrappend information\n",
    "    post_scrappings.append(dict_post_scrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs scrapped: 6\n"
     ]
    }
   ],
   "source": [
    "print(f'URLs scrapped: {len(post_scrappings)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data in mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 6 articles\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client.db.webscrapping\n",
    "try:\n",
    "    db.insert_many(post_scrappings)\n",
    "    print(f'Inserted {len(post_scrappings)} articles')\n",
    "except:\n",
    "    print('an error occurred quotes were not stored to db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c633461e12ddbd51a02e1735af6afd1d7dd20effaf171e9e2afa76d128a40c4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
